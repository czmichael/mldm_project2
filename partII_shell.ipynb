{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Model Building\n",
    "\n",
    "Here you try your hand at model building to predict appointment no shows.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Package 'proj2_lib' now includes code to carry out preprocessing steps from part I. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.util as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it includes a dictionary used for configuring path and file names\n",
    "used through the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_pipeline_file': 'feature_pipeline.pkl',\n",
       " 'labels_pipeline_file': 'labels_pipeline.pkl',\n",
       " 'objstore_path': 'objects',\n",
       " 'processed_data_path': 'processed_data',\n",
       " 'raw_data_csv': 'KaggleV2-May-2016.csv',\n",
       " 'raw_data_path': 'data',\n",
       " 'test_csv': 'test_set.csv',\n",
       " 'train_csv': 'train_set.csv'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.file_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_pipeline_file`: file storing the preprocessing pipeline used for preparing the feature matrix\n",
    "\n",
    "`labels_pipeline_file`: file storing the preprocessing pipeline used for\n",
    "preparing labels\n",
    "\n",
    "`objstore_path`: directory to store python objects to disk\n",
    "\n",
    "`processed_data_path`: directory containing processed data\n",
    "\n",
    "`raw_data_csv`: name of the csv download from Kaggle\n",
    "\n",
    "`raw_data_path`: directory containing raw data\n",
    "\n",
    "`test_csv`: name of csv file containing test set\n",
    "\n",
    "`train_csv`: name of csv file containing train set\n",
    "\n",
    "You can change these paths and names to suit your project directory structure if you need so. E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_config = utils.file_config\n",
    "#config['raw_data_path'] = \"some_other_directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to create train test sets. Code is in file `proj2_lib/util.py` function `make_train_test_sets`. You\n",
    "can edit that function as needed to include your own part I code if you so desire. The result will be to \n",
    "create files `train_set.csv` and `test_set.csv` in your `processed_data` directory (unless you change any of the entries in the configuration directory as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ONLY NEED TO RUN THIS STEP ONCE (switch this to True to run it)\n",
    "RUN_MAKE_TRAIN_TEST_FILES = False\n",
    "if RUN_MAKE_TRAIN_TEST_FILES:\n",
    "    utils.make_train_test_sets(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to fit the preprocessing pipelines. This is done in file `proj2_lib/preprocess.py`. Again you can edit code as needed in that file to incorporate your part I solution as you wish. The result will be to create files `feature_pipeline.pkl` and `labels_pipeline.pkl` containing the fit preprocessing pipelines we can then use to preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.preprocess as preprocess\n",
    "\n",
    "# ONLY NEED TO RUN THIS STEP ONCE\n",
    "RUN_FIT_PREPROCESSING = False\n",
    "if RUN_FIT_PREPROCESSING:\n",
    "    preprocess.fit_save_pipelines(config=file_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we do that, we can get a training matrix and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90526, 101)\n",
      "(90526,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "Using `sklearn` fit:\n",
    "    - DecisionTree classifier\n",
    "    - RandomForest classifier\n",
    "    - Linear SVM classifier\n",
    "    - SVM with Radial Basis Kernel classifier\n",
    "    \n",
    "Use default parameters for now.\n",
    "Using 10-fold cross validation report both accuracy and AUC for each of the above four models.\n",
    "\n",
    "QUESTION: Should you use accuracy or AUC for this task as a performance metric?\n",
    "\n",
    "_ANSWER HERE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "PROCESSED_DATA_DIR = 'processed_data'\n",
    "#======================   Data PreProcessing ============================\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder, StandardScaler\n",
    "\n",
    "# github.com/pandas-dev/sklearn-pandas\n",
    "# install with pip install sklearn-pandas\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "class YesNoTransform(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        no_show = np.ndarray(shape=X.shape, dtype=int)\n",
    "        for index, rowdata in X.iterrows():\n",
    "            if any(rowdata == 'Yes'):\n",
    "                no_show[index] = 1\n",
    "            if any(rowdata == 'No'):\n",
    "                no_show[index] = -1\n",
    "        return no_show\n",
    "\n",
    "\n",
    "yes_no_mapper = DataFrameMapper([\n",
    "    (['No-show'], YesNoTransform())\n",
    "], input_df=True)\n",
    "    \n",
    "    \n",
    "no_show_pipeline = Pipeline([\n",
    "    ('yes_no_mapper', yes_no_mapper)\n",
    "])    \n",
    "\n",
    "\n",
    "class WeekdayTransform(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X['AppointmentDay'].dt.weekday.values\n",
    "\n",
    "weekday_mapper = DataFrameMapper([\n",
    "    (['AppointmentDay'], WeekdayTransform())\n",
    "], input_df=True)\n",
    "    \n",
    "\n",
    "weekday_pipeline = Pipeline([\n",
    "    ('weekday_adder', weekday_mapper),\n",
    "    ('weekday_encoder', OneHotEncoder(n_values=7))\n",
    "])\n",
    "\n",
    "\n",
    "class DaysAheadTransform(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        daysahead = (X['AppointmentDay'] - X['ScheduledDay'])\\\n",
    "            .dt.days\\\n",
    "            .values\\\n",
    "            .astype('float64')\n",
    "        return daysahead\n",
    "    \n",
    "daysahead_mapper = DataFrameMapper([\n",
    "    (['AppointmentDay', 'ScheduledDay'], DaysAheadTransform())\n",
    "], input_df=True)\n",
    "\n",
    "daysahead_pipeline = Pipeline([\n",
    "    ('mapper', daysahead_mapper),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "date_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('weekday_pipeline', weekday_pipeline),\n",
    "    ('daysahead_pipeline', daysahead_pipeline)\n",
    "])\n",
    "\n",
    "numeric_attributes = ['Scholarship',\n",
    "                      'Hypertension',\n",
    "                      'Diabetes',\n",
    "                      'Alcoholism',\n",
    "                      'SMS_received'\n",
    "                     ]\n",
    "\n",
    "num_mapper = DataFrameMapper(list(zip(numeric_attributes, [None for x in numeric_attributes])))\n",
    "\n",
    "df_mapper = DataFrameMapper([\n",
    "    (['Age'], StandardScaler()),\n",
    "    ('Gender', LabelBinarizer()),\n",
    "    ('Neighbourhood', LabelBinarizer()),\n",
    "    (['Handicap'], OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('date_pipeline', date_pipeline),\n",
    "    ('num_mapper', num_mapper),\n",
    "    ('df_mapper', df_mapper),\n",
    "    ('no_show_pipeline', no_show_pipeline)\n",
    "])\n",
    "\n",
    "\n",
    "clean_df = pd.read_csv(PROCESSED_DATA_DIR + \"/train_set.csv\", parse_dates=['ScheduledDay','AppointmentDay'],\n",
    "                      dtype={'Age': np.float64})\n",
    "\n",
    "full_pipeline.fit(clean_df)\n",
    "appt_mat = full_pipeline.transform(clean_df)\n",
    "print(type(appt_mat))\n",
    "\n",
    "appt_mat_df = pd.DataFrame(appt_mat.toarray())\n",
    "\n",
    "appt_mat_df.to_csv(PROCESSED_DATA_DIR + '/appt_mat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6         7    8    9 ...    92   93   94  \\\n",
      "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  2.017970  0.0  0.0 ...   0.0  0.0  0.0   \n",
      "1  1.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.339370  0.0  1.0 ...   0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  1.0  0.0  0.0 -0.601297  0.0  1.0 ...   0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0 -0.666779  1.0  0.0 ...   0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  1.0  0.0  0.0 -0.011962  0.0  0.0 ...   1.0  0.0  0.0   \n",
      "\n",
      "    95   96   97   98   99  100  101  \n",
      "0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
      "1  0.0  1.0  0.0  0.0  0.0  0.0 -1.0  \n",
      "2  0.0  1.0  0.0  0.0  0.0  0.0 -1.0  \n",
      "3  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  1.0  0.0  0.0  0.0  0.0 -1.0  \n",
      "\n",
      "[5 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "\n",
    "DATA_10_FOLD_DIR = 'data_10_fold'\n",
    "DATA_5_FOLD_DIR = 'data_5_fold'\n",
    "\n",
    "PROCESSED_DATA_DIR = 'processed_data'\n",
    "CSV_FILE = PROCESSED_DATA_DIR + '/appt_mat.csv'\n",
    "\n",
    "appt_df = pd.read_csv(CSV_FILE)\n",
    "print(appt_df.head())\n",
    "\n",
    "df_feature = appt_df.iloc[:, :101].as_matrix()\n",
    "df_label = appt_df.iloc[:, 101:].as_matrix()\n",
    "\n",
    "K = 11\n",
    "\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = []\n",
    "    train_label = []\n",
    "    test_feature = []\n",
    "    test_label = []\n",
    "    \n",
    "    for n in range(1, df_feature.shape[0]):\n",
    "        if n % K != k - 1:\n",
    "            train_feature.append(df_feature[n].tolist())\n",
    "            train_label.append(df_label[n].tolist())\n",
    "\n",
    "        else:\n",
    "            test_feature.append(df_feature[n].tolist())\n",
    "            test_label.append(df_label[n].tolist())\n",
    "    train_feature_df = pd.DataFrame(train_feature)\n",
    "    train_label_df = pd.DataFrame(train_label)\n",
    "    test_feature_df = pd.DataFrame(test_feature)\n",
    "    test_label_df = pd.DataFrame(test_label)\n",
    "    \n",
    "    \n",
    "    train_feature_df.to_csv(DATA_10_FOLD_DIR + '/train_feature_' + str(k) + '.csv', index=False)\n",
    "    train_label_df.to_csv(DATA_10_FOLD_DIR + '/train_label_' + str(k) + '.csv', index=False)\n",
    "    test_feature_df.to_csv(DATA_10_FOLD_DIR + '/test_feature_' + str(k) + '.csv', index=False)\n",
    "    test_label_df.to_csv(DATA_10_FOLD_DIR + '/test_label_' + str(k) + '.csv', index=False)    \n",
    "\n",
    "K = 6\n",
    "\n",
    "\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = []\n",
    "    train_label = []\n",
    "    test_feature = []\n",
    "    test_label = []\n",
    "    \n",
    "    for n in range(1, df_feature.shape[0]):\n",
    "        if n % K != k - 1:\n",
    "            train_feature.append(df_feature[n].tolist())\n",
    "            train_label.append(df_label[n].tolist())\n",
    "\n",
    "        else:\n",
    "            test_feature.append(df_feature[n].tolist())\n",
    "            test_label.append(df_label[n].tolist())\n",
    "            \n",
    "    train_feature_df = pd.DataFrame(train_feature)\n",
    "    train_label_df = pd.DataFrame(train_label)\n",
    "    test_feature_df = pd.DataFrame(test_feature)\n",
    "    test_label_df = pd.DataFrame(test_label)\n",
    "    \n",
    "    \n",
    "    train_feature_df.to_csv(DATA_5_FOLD_DIR + '/train_feature_' + str(k) + '.csv', index=False)\n",
    "    train_label_df.to_csv(DATA_5_FOLD_DIR + '/train_label_' + str(k) + '.csv', index=False)\n",
    "    test_feature_df.to_csv(DATA_5_FOLD_DIR + '/test_feature_' + str(k) + '.csv', index=False)\n",
    "    test_label_df.to_csv(DATA_5_FOLD_DIR + '/test_label_' + str(k) + '.csv', index=False)   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1, accuracy=0.800340260056, AUC=0.5\n",
      "K=2, accuracy=0.802673147023, AUC=0.5\n",
      "K=3, accuracy=0.788335358445, AUC=0.5\n",
      "K=4, accuracy=0.79489671932, AUC=0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "DATA_10_FOLD_DIR = 'data_10_fold'\n",
    "\n",
    "    \n",
    "K = 11\n",
    "accuracy_total = 0\n",
    "roc_auc_total = 0\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = pd.read_csv(DATA_10_FOLD_DIR + '/train_feature_' + str(k) + '.csv').as_matrix()\n",
    "    train_label = pd.read_csv(DATA_10_FOLD_DIR + '/train_label_' + str(k) + '.csv').as_matrix()\n",
    "    test_feature = pd.read_csv(DATA_10_FOLD_DIR + '/test_feature_' + str(k) + '.csv').as_matrix()\n",
    "    test_label = pd.read_csv(DATA_10_FOLD_DIR + '/test_label_' + str(k) + '.csv').as_matrix()\n",
    "    \n",
    "    \n",
    "    X = train_feature\n",
    "    Y = train_label\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf = clf.fit(X, Y)\n",
    "    \n",
    "    predicted = clf.predict(test_feature)\n",
    "    actual = test_label.flatten()\n",
    "    \n",
    "    accuracy = sum(np.array(predicted)==np.array(actual))/float(len(actual))\n",
    "    fpr, tpr, thresholds = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"K={}, accuracy={}, AUC={}\".format(k, accuracy, roc_auc))\n",
    "\n",
    "    accuracy_total = accuracy_total + accuracy\n",
    "    roc_auc_total = roc_auc_total + roc_auc\n",
    "\n",
    "\n",
    "accuracy_final = accuracy_total / 10\n",
    "roc_auc_final = roc_auc_total / 10\n",
    "\n",
    "print (\"for random forrest, accuracy={}, AUC={}\".format(accuracy_final, roc_auc_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "DATA_10_FOLD_DIR = 'data_10_fold'\n",
    "\n",
    "    \n",
    "K = 11\n",
    "accuracy_total = 0\n",
    "roc_auc_total = 0\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = pd.read_csv(DATA_10_FOLD_DIR + '/train_feature_' + str(k) + '.csv').as_matrix()\n",
    "    train_label = pd.read_csv(DATA_10_FOLD_DIR + '/train_label_' + str(k) + '.csv').as_matrix()\n",
    "    test_feature = pd.read_csv(DATA_10_FOLD_DIR + '/test_feature_' + str(k) + '.csv').as_matrix()\n",
    "    test_label = pd.read_csv(DATA_10_FOLD_DIR + '/test_label_' + str(k) + '.csv').as_matrix()\n",
    "    \n",
    "    X = train_feature\n",
    "    Y = train_label\n",
    "    clf = LinearSVC(random_state=0)\n",
    "    clf = clf.fit(X, Y)\n",
    "    \n",
    "    predicted = clf.predict(test_feature)\n",
    "    actual = test_label.flatten()\n",
    "    \n",
    "    accuracy = sum(np.array(predicted)==np.array(actual))/float(len(actual))\n",
    "    fpr, tpr, thresholds = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"K={}, accuracy={}, AUC={}\".format(k, accuracy, roc_auc))\n",
    "\n",
    "    accuracy_total = accuracy_total + accuracy\n",
    "    roc_auc_total = roc_auc_total + roc_auc\n",
    "\n",
    "\n",
    "accuracy_final = accuracy_total / 10\n",
    "roc_auc_final = roc_auc_total / 10\n",
    "\n",
    "print (\"for Linear SVM, accuracy={}, AUC={}\".format(accuracy_final, roc_auc_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "DATA_10_FOLD_DIR = 'data_10_fold'\n",
    "\n",
    "    \n",
    "K = 11\n",
    "accuracy_total = 0\n",
    "roc_auc_total = 0\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = pd.read_csv(DATA_10_FOLD_DIR + '/train_feature_' + str(k) + '.csv').as_matrix()\n",
    "    train_label = pd.read_csv(DATA_10_FOLD_DIR + '/train_label_' + str(k) + '.csv').as_matrix()\n",
    "    test_feature = pd.read_csv(DATA_10_FOLD_DIR + '/test_feature_' + str(k) + '.csv').as_matrix()\n",
    "    test_label = pd.read_csv(DATA_10_FOLD_DIR + '/test_label_' + str(k) + '.csv').as_matrix()\n",
    "    \n",
    "    X = train_feature\n",
    "    Y = train_label\n",
    "    clf = SVC()\n",
    "    clf = clf.fit(X, Y)\n",
    "    \n",
    "    predicted = clf.predict(test_feature)\n",
    "    actual = test_label.flatten()\n",
    "    \n",
    "    accuracy = sum(np.array(predicted)==np.array(actual))/float(len(actual))\n",
    "    fpr, tpr, thresholds = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"K={}, accuracy={}, AUC={}\".format(k, accuracy, roc_auc))\n",
    "\n",
    "    accuracy_total = accuracy_total + accuracy\n",
    "    roc_auc_total = roc_auc_total + roc_auc\n",
    "\n",
    "\n",
    "accuracy_final = accuracy_total / 10\n",
    "roc_auc_final = roc_auc_total / 10\n",
    "\n",
    "print (\"for Linear SVM, accuracy={}, AUC={}\".format(accuracy_final, roc_auc_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "Based on the above, choose two methods and fit a tuned model:\n",
    "    - use 5-fold cross validation for model selection\n",
    "    - use 10-fold cross validation for model assessment (based on appropriate performance metric)\n",
    "\n",
    "Report estimated performance for both tuned classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "DATA_5_FOLD_DIR = 'data_5_fold'\n",
    "\n",
    "    \n",
    "K = 6\n",
    "accuracy_total = 0\n",
    "roc_auc_total = 0\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = pd.read_csv(DATA_5_FOLD_DIR + '/train_feature_' + str(k) + '.csv').as_matrix()\n",
    "    train_label = pd.read_csv(DATA_5_FOLD_DIR + '/train_label_' + str(k) + '.csv').as_matrix()\n",
    "    test_feature = pd.read_csv(DATA_5_FOLD_DIR + '/test_feature_' + str(k) + '.csv').as_matrix()\n",
    "    test_label = pd.read_csv(DATA_5_FOLD_DIR + '/test_label_' + str(k) + '.csv').as_matrix()\n",
    "    \n",
    "    \n",
    "    X = train_feature\n",
    "    Y = train_label\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X, Y)\n",
    "    \n",
    "    predicted = clf.predict(test_feature)\n",
    "    actual = test_label.flatten()\n",
    "    \n",
    "    accuracy = sum(np.array(predicted)==np.array(actual))/float(len(actual))\n",
    "    fpr, tpr, thresholds = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"K={}, accuracy={}, AUC={}\".format(k, accuracy, roc_auc))\n",
    "\n",
    "    accuracy_total = accuracy_total + accuracy\n",
    "    roc_auc_total = roc_auc_total + roc_auc\n",
    "\n",
    "\n",
    "accuracy_final = accuracy_total / 5\n",
    "roc_auc_final = roc_auc_total / 5\n",
    "\n",
    "print (\"for decision tree with 5 fold cross validation, accuracy={}, AUC={}\".format(accuracy_final, roc_auc_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "class Gradient_Descent(object):\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.w, self.b = self.calculate_gradient_descent(X, Y)\n",
    "        \n",
    "\n",
    "    def calculate_gradient_descent(self, X, Y, num_iter=500, _lambda=1, learn_rate=0.1):\n",
    "        b = 0\n",
    "        w = np.full((X.shape[1], 1), 0.0)\n",
    "        \n",
    "        for k in range(0, num_iter):\n",
    "            print('current iteration -> {}'.format(k))\n",
    "            \n",
    "            # Halve learning rate for every iteration\n",
    "            learn_rate = learn_rate / 2\n",
    "    \n",
    "            #calculate gradient to w and b respectively\n",
    "            gradient_to_w, gradient_to_b = self.calculate_gradient_using_hinge_loss(w, X, Y, _lambda, b)\n",
    "            \n",
    "            # Update b & w\n",
    "            w = w - learn_rate * gradient_to_w\n",
    "            b = b - learn_rate * gradient_to_b\n",
    "                \n",
    "        return w, b\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_gradient_using_hinge_loss(self, w, X, Y, _lambda, b):\n",
    "\n",
    "        gradient_to_w = np.full((1, w.shape[0]), 0.0)\n",
    "        gradient_to_b = 0\n",
    "        \n",
    "        for (x_, y_) in zip(X, Y):\n",
    "            _x = np.full((1, x_.shape[0]), x_)\n",
    "            \n",
    "            f = _x.dot(w) + b\n",
    "            u = y_ * f\n",
    "            \n",
    "            # Using hinge loss ->gradient = -yx if yf < 1, or 0 if yf > 1\n",
    "            if np.any(u < 1):\n",
    "                gradient_w_curr = -y_ * x_ + (2 * _lambda * LA.norm(w))\n",
    "                gradient_b_curr = -y_\n",
    "            else:\n",
    "                gradient_w_curr = np.full((1, w.shape[0]), 0.0)   \n",
    "                gradient_b_curr = 0 \n",
    "                \n",
    "            gradient_to_w += gradient_w_curr\n",
    "            gradient_to_b += gradient_b_curr\n",
    "            \n",
    "        #print(gradient_to_w)\n",
    "        return np.transpose(gradient_to_w), gradient_to_b\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        w = self.w\n",
    "        b = self.b\n",
    "        outcome_list = []\n",
    "        \n",
    "        print(type(X))\n",
    "        \n",
    "        for _x in X:\n",
    "            x = np.full((1, _x.shape[0]), _x)\n",
    "            f = x.dot(w) + b\n",
    "            \n",
    "            if f >= 0:\n",
    "                outcome = 1.0\n",
    "            else:\n",
    "                outcome = -1.0\n",
    "            outcome_list.append(outcome)\n",
    "         \n",
    "        return outcome_list    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from Gradient_Descent import Gradient_Descent\n",
    "from collections import Counter\n",
    "\n",
    "DATA_10_FOLD_DIR = 'data_10_fold'\n",
    "\n",
    "K = 2\n",
    "accuracy_total = 0\n",
    "roc_auc_total = 0\n",
    "for k in range(1, K):\n",
    "    \n",
    "    train_feature = pd.read_csv(DATA_10_FOLD_DIR + '/train_feature_' + str(k) + '.csv').as_matrix()\n",
    "    train_label = pd.read_csv(DATA_10_FOLD_DIR + '/train_label_' + str(k) + '.csv').as_matrix()\n",
    "    test_feature = pd.read_csv(DATA_10_FOLD_DIR + '/test_feature_' + str(k) + '.csv').as_matrix()\n",
    "    test_label = pd.read_csv(DATA_10_FOLD_DIR + '/test_label_' + str(k) + '.csv').as_matrix()\n",
    "    \n",
    "    \n",
    "    X = train_feature\n",
    "    Y = train_label\n",
    "    gradient_descent = Gradient_Descent()\n",
    "    gradient_descent.fit(X, Y)\n",
    "    \n",
    "    predicted = gradient_descent.predict(test_feature)\n",
    "    actual = test_label.flatten()\n",
    "  \n",
    "    print('predicted -->', Counter(predicted))\n",
    "    print('actual -->', Counter(actual))\n",
    "    print('equals -->', sum(np.array(predicted)==np.array(actual)))\n",
    "    \n",
    "    accuracy = sum(np.array(predicted)==np.array(actual))/float(len(actual))\n",
    "    fpr, tpr, thresholds = roc_curve(actual, predicted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    accuracy_total = accuracy_total + accuracy\n",
    "    roc_auc_total = roc_auc_total + roc_auc\n",
    "    \n",
    "\n",
    "accuracy_final = accuracy_total / (K - 1)\n",
    "roc_auc_final = roc_auc_total / (K - 1)\n",
    "\n",
    "print (\"for gradient descent, accuracy={}, AUC={}\".format(accuracy_final, roc_auc_final))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0, obj 82716.70\n",
      "it: 10, obj 57686416.24\n",
      "it: 20, obj 88729263.82\n",
      "it: 30, obj 18878808.66\n",
      "it: 40, obj 29077969.32\n",
      "it: 50, obj 16569926.42\n",
      "it: 60, obj 2649898.07\n",
      "it: 70, obj 1512776.62\n",
      "it: 80, obj 1238395.36\n",
      "it: 90, obj 989114.39\n"
     ]
    }
   ],
   "source": [
    "w,b = fit_svm(train_X, train_y, 1.0, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
