{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part III: Ensembles and Final Result\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Train an AdaBoost classifier using Decision Tree stubs as weak learners. Compare its performance to results obtained in Part II using 10 fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import proj2_lib.util as utils\n",
    "import proj2_lib.preprocess as preprocess\n",
    "\n",
    "\n",
    "file_config = utils.file_config\n",
    "train_X, train_y = preprocess.load_train_data(config=file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaBoost code goes here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Train a decision tree stub\n",
    "tree_classifier = DecisionTreeClassifier(max_depth=2)\n",
    "tree_classifier.fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7972184705094304"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2))\n",
    "\n",
    "scores = cross_val_score(bdt, train_X, train_y, \n",
    "                         scoring='accuracy', cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72824282013900843"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_scores = cross_val_score(bdt, train_X, train_y,\n",
    "                            scoring='roc_auc', cv=10)\n",
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Choose a set of 5 or so classifiers. Write a function that trains an ensemble using stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_stack_ensemble(X, y):\n",
    "    # create train/validation sets\n",
    "    # using StratifiedShuffleSplit\n",
    "\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "#          print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    # train classifiers in ensemble using train set\n",
    "    tc_1 = DecisionTreeClassifier(max_depth=5)\n",
    "    tc_1.fit(X, y)\n",
    "    \n",
    "    tc_2 = DecisionTreeClassifier(max_depth=30)\n",
    "    tc_2.fit(X, y)\n",
    "    \n",
    "    linear_svm = LinearSVC(dual=False)\n",
    "    linear_svm.fit(X, y)\n",
    "    \n",
    "    rf_1 = RandomForestClassifier(n_estimators=5)\n",
    "    rf_1.fit(X, y)\n",
    " \n",
    "    rf_2 = RandomForestClassifier(n_estimators=15)\n",
    "    rf_2.fit(X, y)\n",
    "    \n",
    "    # create new feature matrix for validation\n",
    "    # set by getting predictions from the ensemble\n",
    "    # classifiers\n",
    "\n",
    "    tc_1_predict = tc_1.predict(X_test)\n",
    "    tc_2_predict = tc_2.predict(X_test)\n",
    "    linear_svm_predict = linear_svm.predict(X_test)\n",
    "    rf_1_predict = rf_1.predict(X_test)\n",
    "    rf_2_predict = rf_2.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # train logistic regression classifier on\n",
    "    # new feature matrix\n",
    "    X_ = np.array([tc_1_predict,\n",
    "                   tc_2_predict,\n",
    "                   linear_svm_predict,\n",
    "                   rf_1_predict,\n",
    "                   rf_2_predict])\n",
    "    \n",
    "    _X = np.transpose(X_)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(_X, y_test)    \n",
    "    \n",
    "    # return all trained classifiers\n",
    "    return tc_1, tc_2, linear_svm, rf_1, rf_2, logreg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 10-fold cross validation to measure performance of your stacked classifier. See Part II solution to see how to roll your own sklearn classifier along with http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logref = ''\n",
    "        self.f1 = ''\n",
    "        self.f2 = ''\n",
    "        self.f3 = ''\n",
    "        self.f4 = ''\n",
    "        self.f5 = ''\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.f1, self.f2, self.f3, self.f4, self.f5, self.logref = build_stack_ensemble(X, y)\n",
    "        return self\n",
    "    \n",
    "    def get_feature(self, X):\n",
    "        pred_1 = self.f1.predict(X)\n",
    "        pred_2 = self.f2.predict(X)\n",
    "        pred_3 = self.f3.predict(X)\n",
    "        pred_4 = self.f4.predict(X)\n",
    "        pred_5 = self.f5.predict(X)\n",
    "        \n",
    "        \n",
    "        X_ = np.array([pred_1,\n",
    "                       pred_2,\n",
    "                       pred_3,\n",
    "                       pred_4,\n",
    "                       pred_5])\n",
    "    \n",
    "        _X = np.transpose(X_)\n",
    "        return _X;\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _X = self.get_feature(X)\n",
    "        return self.logref.predict(_X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        _X = self.get_feature(X)\n",
    "        return self.logref.predict_proba(_X)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = EnsembleClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76848633292655388"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(ec, train_X, train_y, scoring='accuracy', cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611904995843644"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_scores = cross_val_score(ec, train_X, train_y, scoring='roc_auc', cv=10)\n",
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Choose a single model based on all previous project steps. Train this model on the complete training dataset and measure it's performance on the held out test set.\n",
    "\n",
    "Compare to the 10-fold CV estimate you got previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7813, roc_auc = 0.567455064515\n"
     ]
    }
   ],
   "source": [
    "# final result goes here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_X, test_y = preprocess.load_test_data(config=file_config)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=30, max_features=30)\n",
    "rf_classifier.fit(train_X, train_y)\n",
    "text_y_predict = rf_classifier.predict(test_X)\n",
    "\n",
    "accuracy = accuracy_score(test_y, text_y_predict)\n",
    "auc = roc_auc_score(test_y, text_y_predict)\n",
    "\n",
    "print('accuracy = {}, roc_auc = {}'.format(accuracy, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
